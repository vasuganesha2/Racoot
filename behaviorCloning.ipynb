{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad70953c",
   "metadata": {},
   "source": [
    "# Behavior Cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e64b3c",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea3789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRacingDataset_RNN(Dataset):\n",
    "    def __init__(self, images, labels, sequence_length, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Extract a sequence of frames\n",
    "        start_idx = index\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "        sequence = self.images[start_idx:end_idx]\n",
    "        #sequence = np.transpose(sequence, (0, 3, 1, 2))\n",
    "        label_sequence = self.labels[start_idx:end_idx]\n",
    "\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            # If the sequence is shorter than the specified length, pad it\n",
    "            padding_frames = [np.zeros_like(sequence[0])] * (self.sequence_length - len(sequence))\n",
    "            sequence = np.concatenate([sequence, padding_frames])\n",
    "            # You might need to handle padding for labels as well based on your requirement\n",
    "            last_label = label_sequence[-1]\n",
    "            padding_labels = [last_label] * (self.sequence_length - len( label_sequence))\n",
    "\n",
    "            label_sequence = np.concatenate((label_sequence,padding_labels), axis=0)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            transformed_sequence = [self.transform(frame) for frame in sequence]\n",
    "            # Leave the transformed frames as PyTorch tensors\n",
    "            sequence = torch.stack(transformed_sequence)\n",
    "\n",
    "        return sequence, label_sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) - self.sequence_length\n",
    "\n",
    "\n",
    "\n",
    "class CarRacingDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None, grayscale = False):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.grayscale = grayscale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.fromarray(self.images[idx])\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        if self.grayscale:\n",
    "            image = image.convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ca3be",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c593ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a67c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_RNN_Classifier(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_size=None, rnn_hidden_size=64, rnn_num_layers=1):\n",
    "        super(CNN_RNN_Classifier, self).__init__()\n",
    "\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn_num_layers = rnn_num_layers\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 22 * 22, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnn = nn.LSTM(64 * 22 * 22, rnn_hidden_size, rnn_num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc2 = nn.Linear(rnn_hidden_size, out_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()  # (batch,seq,3,96,96)\n",
    "        c_in = x.view(batch_size * seq_len, c, h, w).float()\n",
    "\n",
    "        x = self.relu(self.conv1(c_in))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool1(x)  # (batch size * seq, 64, 22, 22)\n",
    "        x = x.reshape(batch_size * seq_len, -1)\n",
    "        # rnn_input = self.relu(self.dropout(self.fc1(x)))\n",
    "        rnn_input = self.relu(self.dropout(x))\n",
    "        rnn_input = rnn_input.view(batch_size, seq_len, -1)\n",
    "        rnn_out, _ = self.rnn(rnn_input)\n",
    "\n",
    "        output = self.fc2(rnn_out)  # [batch, seq, 4]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CNNClassifier(nn.Module):  # Architecture\n",
    "    def __init__(self, in_channels=None, out_size=None):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 22 * 22, 512)\n",
    "        self.fc2 = nn.Linear(512, out_size)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.relu(self.dropout(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ce683",
   "metadata": {},
   "source": [
    "## Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ac4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c02debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(data_loader, num_samples=30):\n",
    "\n",
    "    for images, actions in data_loader:\n",
    "\n",
    "        rows = int(np.sqrt(num_samples))\n",
    "        cols = int(np.ceil(num_samples / rows))\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            observation = images[i]\n",
    "            action = actions[i].tolist()\n",
    "            plt.imshow(np.transpose(observation, (1, 2, 0)), interpolation='nearest')\n",
    "            plt.title(f\"Action: {action}\", fontsize=7)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "\n",
    "class ChangeColorTransform:\n",
    "\n",
    "    def __call__(self, img):\n",
    "\n",
    "        img = np.array(img)\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        mask_grey = cv2.inRange(hsv, (0, 0, 0.2), (180, 0.1, 0.5))  # Mask for grey color\n",
    "\n",
    "        img[mask_grey > 0] = (50, 0.5, 0.3)  # Grey to Brown color\n",
    "\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = torch.from_numpy(img).float()\n",
    "\n",
    "        return img\n",
    "\n",
    "class RandomCropAndRotation:\n",
    "    def __init__(self, crop_size=(80, 80), rotation_angle=20):\n",
    "        self.crop_size = crop_size\n",
    "        self.rotation_angle = rotation_angle\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Random crop\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(img, output_size=self.crop_size)\n",
    "        img = transforms.functional.crop(img, i, j, h, w)\n",
    "\n",
    "        # Random rotation\n",
    "        angle = np.random.uniform(-self.rotation_angle, self.rotation_angle)\n",
    "        img = transforms.functional.rotate(img, angle)\n",
    "\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7143a18b",
   "metadata": {},
   "source": [
    "## Data Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef2f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keyboard\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f16a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_action(action):\n",
    "    \"\"\"\n",
    "    Map continuous [steer, gas, brake] to one-hot 5-dim:\n",
    "      [left, right, accel, brake, no-op]\n",
    "    \"\"\"\n",
    "    if   action[0] == -0.5: return [1, 0, 0, 0, 0]\n",
    "    elif action[0] ==  0.5: return [0, 1, 0, 0, 0]\n",
    "    elif action[1] ==  0.5: return [0, 0, 1, 0, 0]\n",
    "    elif action[2] ==  0.5: return [0, 0, 0, 1, 0]\n",
    "    else:                  return [0, 0, 0, 0, 1]\n",
    "\n",
    "def collect_manual_data(env,\n",
    "                        num_episodes: int,\n",
    "                        frames_per_ep: int,\n",
    "                        trim_initial: int,\n",
    "                        raw_out_path: str):\n",
    "    \"\"\"\n",
    "    Plays `num_episodes` sequentially under human control, collects frames,\n",
    "    drops the first `trim_initial` per episode, and writes one .npz.\n",
    "    \"\"\"\n",
    "    all_imgs, all_labels = [], []\n",
    "\n",
    "    for ep in range(1, num_episodes+1):\n",
    "        print(f\"[Episode {ep}/{num_episodes}]\")\n",
    "        imgs_buf, labs_buf = [], []\n",
    "        env.reset()\n",
    "\n",
    "        for t in range(frames_per_ep):\n",
    "            action = [0.0, 0.0, 0.0]\n",
    "            if   keyboard.is_pressed('left'):  action[0] = -0.5\n",
    "            elif keyboard.is_pressed('right'): action[0] =  0.5\n",
    "            elif keyboard.is_pressed('up'):    action[1] =  0.5\n",
    "            elif keyboard.is_pressed('down'):  action[2] =  0.5\n",
    "\n",
    "            obs, _, done, _, _ = env.step(action)\n",
    "            env.render()\n",
    "\n",
    "            cropped = obs[0:82, 0:96]\n",
    "            imgs_buf.append(cropped)\n",
    "            labs_buf.append(get_discrete_action(action))\n",
    "\n",
    "            if done or keyboard.is_pressed('q'):\n",
    "                print(\"  → terminated early\")\n",
    "                break\n",
    "\n",
    "        # drop warm-up frames\n",
    "        imgs_buf = imgs_buf[trim_initial:]\n",
    "        labs_buf = labs_buf[trim_initial:]\n",
    "        all_imgs.extend(imgs_buf)\n",
    "        all_labels.extend(labs_buf)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    np.savez(raw_out_path,\n",
    "             images=np.array(all_imgs, dtype=np.uint8),\n",
    "             labels=np.array(all_labels, dtype=float))\n",
    "    print(f\"[Saved raw data] {raw_out_path}\")\n",
    "\n",
    "def prepare_training_data(raw_path: str,\n",
    "                          train_out_path: str,\n",
    "                          resize_to=(96,96)):\n",
    "    \"\"\"\n",
    "    Loads raw .npz, resizes all frames, drops the no-op class,\n",
    "    and writes the 4-class training .npz.\n",
    "    \"\"\"\n",
    "    data = np.load(raw_path)\n",
    "    imgs = data['images']   # shape: (N, H, W, 3)\n",
    "    labs = data['labels']   # shape: (N, 5)\n",
    "\n",
    "    N = len(labs)\n",
    "    H, W = resize_to\n",
    "    resized = np.zeros((N, H, W, 3), dtype=np.uint8)\n",
    "\n",
    "    for i in range(N):\n",
    "        pil = Image.fromarray(imgs[i])\n",
    "        resized[i] = np.array(pil.resize((W, H)))\n",
    "\n",
    "    # keep only actions [0–3], drop no-op (index 4)\n",
    "    mask = labs[:,4] != 1\n",
    "    final_imgs = resized[mask]\n",
    "    final_labs = labs[mask, :4]\n",
    "\n",
    "    np.savez(train_out_path,\n",
    "             images=final_imgs,\n",
    "             labels=final_labs)\n",
    "    print(f\"[Saved training data] {train_out_path}\")\n",
    "    print(f\"[Final dataset size] {len(final_labs)} examples\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ───────── USER SETTINGS ─────────\n",
    "    NUM_EPISODES       = 18\n",
    "    FRAMES_PER_EPISODE = 800\n",
    "    TRIM_INITIAL       = 30\n",
    "    RAW_FILE           = \"manual_control_all_eps.npz\"\n",
    "    TRAIN_FILE         = \"training_data_4class_all_eps.npz\"\n",
    "    RESIZE_DIMS        = (96, 96)\n",
    "    # ──────────────────────────────────\n",
    "\n",
    "    env = gym.make('CarRacing-v2', render_mode='human')\n",
    "\n",
    "    # 1) collect & save one raw file for all episodes\n",
    "    collect_manual_data(env,\n",
    "                        num_episodes=NUM_EPISODES,\n",
    "                        frames_per_ep=FRAMES_PER_EPISODE,\n",
    "                        trim_initial=TRIM_INITIAL,\n",
    "                        raw_out_path=RAW_FILE)\n",
    "\n",
    "    # 2) process that raw file into your final training set\n",
    "    prepare_training_data(RAW_FILE,\n",
    "                          TRAIN_FILE,\n",
    "                          resize_to=RESIZE_DIMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b06ea",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87d69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Resize, RandomHorizontalFlip, RandomVerticalFlip, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37cfd8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "\n",
    "def train(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    training_acc = 0\n",
    "    run_loss = 0\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "\n",
    "    for batch_idx, (input, label) in enumerate(loop):\n",
    "        input = input.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)  # input one batch to model\n",
    "\n",
    "        loss = loss_fn(output.view(-1, output.size(2)),\n",
    "                       label.argmax(dim=2).view(-1).long())  # Calculates average loss of batch\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        run_loss += loss.item() * input.size(0)\n",
    "        _, predicted = torch.max(output.data, 2)\n",
    "        total_train += label.view(-1, label.size(2)).size(0)\n",
    "        correct_train += (predicted == label.argmax(dim=2)).sum().item()\n",
    "\n",
    "    training_acc += (100 * correct_train / total_train)\n",
    "    training_loss += (run_loss / len(dataloader))  # mean loss for all batches\n",
    "\n",
    "    return training_loss, training_acc\n",
    "\n",
    "\n",
    "def validation(model, val_loader, loss_fn):  # VALIDATION\n",
    "    model.eval()  # evaluation mode\n",
    "    validation_loss = 0\n",
    "    validation_acc = 0\n",
    "    total_test = 0\n",
    "    correct_test = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loop = tqdm(val_loader, leave=True)\n",
    "        for batch_idx, (val_input, val_label) in enumerate(val_loop):\n",
    "            val_input, val_label = val_input.to(DEVICE), val_label.to(DEVICE)\n",
    "\n",
    "            val_output = model(val_input)\n",
    "            loss = loss_fn(val_output.view(-1, val_output.size(2)), val_label.argmax(dim=2).view(-1).long())\n",
    "            val_loss += loss.item() * val_input.size(0)\n",
    "            _, predicted = torch.max(val_output.data, 2)\n",
    "            total_test += val_label.view(-1, val_label.size(2)).size(0)\n",
    "            correct_test += (predicted == val_label.argmax(dim=2)).sum().item()\n",
    "\n",
    "    validation_acc += (100 * correct_test / total_test)\n",
    "    validation_loss += (val_loss / len(val_loader))\n",
    "\n",
    "    return validation_loss, validation_acc\n",
    "\n",
    "\n",
    "def test(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    total_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input, label in test_loader:\n",
    "            input, label = input.to(DEVICE), label.to(DEVICE)\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output.view(-1, output.size(2)), label.argmax(dim=2).view(-1).long())\n",
    "            test_loss += loss.item() * input.size(0)\n",
    "            _, predicted = torch.max(output.data, 2)\n",
    "            total_test += label.view(-1, label.size(2)).size(0)\n",
    "            correct_test += (predicted == label.argmax(dim=2)).sum().item()\n",
    "\n",
    "            # predicted_flat = predicted.view(-1).cpu().numpy()\n",
    "            # label_flat = label.argmax(dim=2).view(-1).cpu().numpy()\n",
    "            #\n",
    "            # precision = precision_score(label_flat, predicted_flat, average='weighted')\n",
    "            # recall = recall_score(label_flat, predicted_flat, average='weighted')\n",
    "            # f1 = f1_score(label_flat, predicted_flat, average='weighted')\n",
    "            #\n",
    "            # print(\"Precision: {:.4f}\".format(precision))\n",
    "            # print(\"Recall: {:.4f}\".format(recall))\n",
    "            # print(\"F1 Score: {:.4f}\".format(f1))\n",
    "\n",
    "    accuracy = (100 * correct_test / total_test)\n",
    "    total_loss += test_loss / len(test_loader)\n",
    "\n",
    "    return accuracy, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = CNN_RNN_Classifier(in_channels=3, out_size=4).to(DEVICE)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCH = 5\n",
    "\n",
    "    TRAIN_4 = True\n",
    "    TEST_4 = True\n",
    "    PLOT = True\n",
    "\n",
    "    data = np.load(\"../RL-proj/training_data_4class_all_eps.npz\")\n",
    "    images = data['images']\n",
    "    labels = data['labels'].astype(float)\n",
    "\n",
    "    # Train data\n",
    "    split_ratio = 0.8\n",
    "    split_index = int(len(images) * split_ratio)\n",
    "\n",
    "    # 4 class\n",
    "    X_train, X_temp = images[:split_index], images[split_index:]\n",
    "    y_train, y_temp = labels[:split_index], labels[split_index:]\n",
    "\n",
    "    # Validation, Test data\n",
    "    split_ratio_val_test = 0.5 \n",
    "    split_index_val = int(len(X_temp) * split_ratio_val_test)\n",
    "\n",
    "    # 4 class\n",
    "    X_val, X_test = X_temp[:split_index_val], X_temp[split_index_val:]\n",
    "    y_val, y_test = y_temp[:split_index_val], y_temp[split_index_val:]\n",
    "\n",
    "    transform_list = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    train_dataset_RNN = CarRacingDataset_RNN(X_train, y_train, sequence_length=5, transform=transform_list)\n",
    "    train_loader_RNN = DataLoader(train_dataset_RNN, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset_RNN = CarRacingDataset_RNN(X_val, y_val, sequence_length=5, transform=transform_list)\n",
    "    val_loader_RNN = DataLoader(val_dataset_RNN, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    test_dataset_RNN = CarRacingDataset_RNN(X_test, y_test, sequence_length=5, transform=transform_list)\n",
    "    test_loader_RNN = DataLoader(test_dataset_RNN, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    val_losses, train_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "    if TRAIN_4:\n",
    "\n",
    "        best_acc = 0\n",
    "        for i in range(EPOCH):\n",
    "            train_loss, train_acc = train(model, train_loader_RNN, loss_fn, optimizer)\n",
    "            val_loss, val_acc = validation(model, val_loader_RNN, loss_fn)\n",
    "\n",
    "            print(f\"EPOCH: {i+1} has validation accuracy of {val_acc:.2f} and loss of {val_loss:.4f}\")\n",
    "            if val_acc > best_acc:  # best accuracy out of all epochs\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), 'weights/modelLSTM_new.pth')\n",
    "\n",
    "                # Load the best model weights\n",
    "            print(\"Saving best model with accuracy: \", best_acc)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "    if PLOT:\n",
    "        # loss plot\n",
    "        plt.figure(figsize=(15, 60))\n",
    "\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'Car Data loss plot, BATCH SIZE={BATCH_SIZE}, EPOCH = {EPOCH}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # accuracy plot\n",
    "\n",
    "        plt.figure(figsize=(10, 40))\n",
    "\n",
    "        plt.plot(train_accs, label='Train Accuracy')\n",
    "        plt.plot(val_accs, label='Validation Accuracy')\n",
    "        plt.title(f'Car Data accuracy plot, BATCH SIZE={BATCH_SIZE}, EPOCH = {EPOCH}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    if TEST_4:\n",
    "\n",
    "        if os.path.exists('weights/modelLSTM_new.pth'):\n",
    "            model.load_state_dict(torch.load('weights/modelLSTM_new.pth'))\n",
    "        acc, loss = test(model, test_loader_RNN, loss_fn)\n",
    "        print(\"Test accuracy(4 class): \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eb52fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6d938",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb9c80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path= './weights/modelLSTM.pth'\n",
    "\n",
    "model = CNN_RNN_Classifier(in_channels=3, out_size=4).to(\"cpu\")\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# transform for input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_observation(observation, prev_obs_buffer):\n",
    "    obs_array = observation[0:82, 0:96]  # crop\n",
    "    obs_tensor = transform(obs_array)\n",
    "    obs_tensor = obs_tensor.unsqueeze(0)  # Add batch dim\n",
    "\n",
    "    # Append current observation to buffer\n",
    "    prev_obs_buffer = torch.cat([prev_obs_buffer[1:], obs_tensor], dim=0)\n",
    "\n",
    "    return prev_obs_buffer\n",
    "\n",
    "\n",
    "def get_model_action(observation):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        output = model(observation.unsqueeze(0))  # Add batch dim\n",
    "        actions = torch.argmax(output, dim=2).squeeze().tolist()\n",
    "        for action in actions:\n",
    "            if action == 0:\n",
    "                return [-0.2, 0.0, 0.0]\n",
    "            if action == 1:\n",
    "                return [0.2, 0.0, 0.0]\n",
    "            if action == 2:\n",
    "                return [0.0, 0.2, 0.0]\n",
    "            if action == 3:\n",
    "                return [0.0, 0.0, 0.2]\n",
    "            else:\n",
    "                return [0.0, 0.0, 0.0]\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make('CarRacing-v2', render_mode='human')\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs_buffer = torch.zeros(5, 3, 96, 96)\n",
    "\n",
    "    for frame in range(1, 5000):  # frames\n",
    "        if frame <= 30:\n",
    "            # random actions for the first 30 frames\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "\n",
    "            obs_buffer = preprocess_observation(obs, obs_buffer)\n",
    "            action = get_model_action(obs_buffer)\n",
    "\n",
    "        obs, _, done, _, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        if keyboard.is_pressed('q' or 'Q'):\n",
    "            break\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfc734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
