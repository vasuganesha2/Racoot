{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5a8e26",
   "metadata": {},
   "source": [
    "# Race Car Behavior Optimization using Soft Actor Critic with Chain of Thoughts as Memory Carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ceffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gym\n",
    "from gym import RewardWrapper\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "\n",
    "\n",
    "np.bool8 = np.bool_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1926d7",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c02b12",
   "metadata": {},
   "source": [
    "### Experiment-1: SAC with Replay Buffer\n",
    "\n",
    "Got a **Reward** of 605.15 after convergence. But, Can we do better? -- **This question inspired us to read about Prioritized Replay Buffer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822653db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Replay buffer ###########\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, capacity):\n",
    "#         self.capacity = capacity\n",
    "#         self.buffer = []\n",
    "#         self.position = 0\n",
    "\n",
    "#     def push(self, state, action, reward, next_state, done):\n",
    "#         if len(self.buffer) < self.capacity:\n",
    "#             self.buffer.append(None)\n",
    "#         self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         batch = random.sample(self.buffer, batch_size)\n",
    "#         states, actions, rewards, next_states, dones = zip(*batch)\n",
    "#         return (\n",
    "#             np.array(states),\n",
    "#             np.array(actions),\n",
    "#             np.array(rewards, dtype=np.float32),\n",
    "#             np.array(next_states),\n",
    "#             np.array(dones, dtype=np.float32),\n",
    "#         )\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.buffer)\n",
    "\n",
    "# ########### Networks ###########\n",
    "# class Actor(nn.Module):\n",
    "#     def __init__(self, state_shape, action_dim):\n",
    "#         super().__init__()\n",
    "#         c, h, w = state_shape\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(c, 32, 8, 4), nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, 3, 1), nn.ReLU(),\n",
    "#         )\n",
    "#         def conv_out(sz, k, s): return (sz - (k-1) - 1)//s + 1\n",
    "#         convw = conv_out(conv_out(conv_out(w, 8, 4), 4, 2), 3, 1)\n",
    "#         convh = conv_out(conv_out(conv_out(h, 8, 4), 4, 2), 3, 1)\n",
    "#         lin = convw * convh * 64\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(lin, 512), nn.ReLU(),\n",
    "#             nn.Linear(512, 512), nn.ReLU()\n",
    "#         )\n",
    "#         self.mean    = nn.Linear(512, action_dim)\n",
    "#         self.log_std = nn.Linear(512, action_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x).flatten(1)\n",
    "#         x = self.fc(x)\n",
    "#         return self.mean(x), self.log_std(x).clamp(-20, 2)\n",
    "\n",
    "#     def sample(self, x):\n",
    "#         mean, log_std = self.forward(x)\n",
    "#         std = log_std.exp()\n",
    "#         dist = Normal(mean, std)\n",
    "#         z = dist.rsample()\n",
    "#         raw_action = torch.tanh(z)  # in [-1,1]\n",
    "\n",
    "#         # carve out steer, gas, brake:\n",
    "#         steer = raw_action[:, 0:1]\n",
    "#         gas   = (raw_action[:, 1:2] + 1.0) / 2.0   # map [-1,1] → [0,1]\n",
    "#         brake = (raw_action[:, 2:3] + 1.0) / 2.0   # map [-1,1] → [0,1]\n",
    "#         action = torch.cat([steer, gas, brake], dim=1)\n",
    "\n",
    "#         logp = (\n",
    "#             dist.log_prob(z)\n",
    "#             - torch.log(1 - raw_action.pow(2) + 1e-6)\n",
    "#         ).sum(1, keepdim=True)\n",
    "#         return action, logp\n",
    "\n",
    "# class Critic(nn.Module):\n",
    "#     def __init__(self, state_shape, action_dim):\n",
    "#         super().__init__()\n",
    "#         c, h, w = state_shape\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(c, 32, 8, 4), nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, 3, 1), nn.ReLU(),\n",
    "#         )\n",
    "#         def conv_out(sz, k, s): return (sz - (k-1) - 1)//s + 1\n",
    "#         convw = conv_out(conv_out(conv_out(w, 8, 4), 4, 2), 3, 1)\n",
    "#         convh = conv_out(conv_out(conv_out(h, 8, 4), 4, 2), 3, 1)\n",
    "#         lin = convw * convh * 64\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(lin + action_dim, 512), nn.ReLU(),\n",
    "#             nn.Linear(512, 512), nn.ReLU(),\n",
    "#             nn.Linear(512, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, a):\n",
    "#         x = self.conv(x).flatten(1)\n",
    "#         return self.fc(torch.cat([x, a], dim=1))\n",
    "\n",
    "# ########### Utils ###########\n",
    "# def soft_update(tgt, src, tau):\n",
    "#     for t, s in zip(tgt.parameters(), src.parameters()):\n",
    "#         t.data.copy_(tau * s.data + (1 - tau) * t.data)\n",
    "\n",
    "# def hard_update(tgt, src):\n",
    "#     tgt.load_state_dict(src.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1a289",
   "metadata": {},
   "source": [
    "### Experiment-2: SAC with Prioritized Experience Replay/ Replay Buffer\n",
    "\n",
    "Got a **Reward** of 791.11 after convergence of 2000 episodes. So, we analyzed that this is a very good improvement from the previous experimentation. Still, we weren't satisfied with the results. A question was still there. Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PrioritizedReplayBuffer:\n",
    "#     def __init__(self, capacity, state_shape, action_shape, device,\n",
    "#                  alpha=0.6, beta_start=0.4, beta_frames=100000, n_step=3, gamma=0.99):\n",
    "#         self.capacity = capacity\n",
    "#         self.device = device\n",
    "#         self.alpha = alpha\n",
    "#         self.beta_start = beta_start\n",
    "#         self.beta_frames = beta_frames\n",
    "#         self.frame_idx = 1\n",
    "#         self.n_step = n_step\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#         # Circular buffer storage\n",
    "#         self.states = torch.zeros((capacity, *state_shape), dtype=torch.float32, device=device)\n",
    "#         self.actions = torch.zeros((capacity, *action_shape), dtype=torch.float32, device=device)\n",
    "#         self.rewards = torch.zeros((capacity,), dtype=torch.float32, device=device)\n",
    "#         self.next_states = torch.zeros((capacity, *state_shape), dtype=torch.float32, device=device)\n",
    "#         self.dones = torch.zeros((capacity,), dtype=torch.bool, device=device)\n",
    "#         self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "#         # N-step buffer\n",
    "#         self.n_step_buffer = []\n",
    "#         self.position = 0\n",
    "#         self.size = 0\n",
    "\n",
    "#     def beta_by_frame(self):\n",
    "#         return min(1.0, self.beta_start + (1.0 - self.beta_start) * self.frame_idx / self.beta_frames)\n",
    "\n",
    "#     def add(self, state, action, reward, next_state, done):\n",
    "#         # Append to n-step buffer\n",
    "#         self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "#         if len(self.n_step_buffer) < self.n_step and not done:\n",
    "#             return\n",
    "\n",
    "#         # Compute multi-step return\n",
    "#         cum_reward, next_s, done_flag = 0.0, None, False\n",
    "#         for idx, (_, _, r, s_next, d) in enumerate(self.n_step_buffer):\n",
    "#             cum_reward += (self.gamma**idx) * r\n",
    "#             next_s, done_flag = s_next, d\n",
    "#             if d:\n",
    "#                 break\n",
    "\n",
    "#         state0, action0, _, _, _ = self.n_step_buffer[0]\n",
    "\n",
    "#         # Store transition\n",
    "#         self.states[self.position] = torch.tensor(state0, dtype=torch.float32, device=self.device)\n",
    "#         self.actions[self.position] = torch.tensor(action0, dtype=torch.float32, device=self.device)\n",
    "#         self.rewards[self.position] = cum_reward\n",
    "#         self.next_states[self.position] = torch.tensor(next_s, dtype=torch.float32, device=self.device)\n",
    "#         self.dones[self.position] = done_flag\n",
    "#         self.priorities[self.position] = self.priorities.max() if self.size > 0 else 1.0\n",
    "\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "#         self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "#         # Remove oldest\n",
    "#         self.n_step_buffer.pop(0)\n",
    "#         if done:\n",
    "#             self.n_step_buffer.clear()\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         assert self.size > 0, \"Empty buffer\"\n",
    "#         self.frame_idx += 1\n",
    "\n",
    "#         # Compute sampling distribution\n",
    "#         prios = self.priorities[:self.size] + 1e-6\n",
    "#         probs = prios ** self.alpha\n",
    "#         probs /= probs.sum()\n",
    "\n",
    "#         # Sample indices\n",
    "#         indices = np.random.choice(self.size, batch_size, p=probs)\n",
    "\n",
    "#         # Importance sampling weights\n",
    "#         beta = self.beta_by_frame()\n",
    "#         weights = (self.size * probs[indices]) ** (-beta)\n",
    "#         weights /= weights.max()\n",
    "#         weights = torch.tensor(weights, dtype=torch.float32, device=self.device)\n",
    "\n",
    "#         # Gather samples\n",
    "#         states = self.states[indices]\n",
    "#         actions = self.actions[indices]\n",
    "#         rewards = self.rewards[indices].unsqueeze(1)\n",
    "#         next_states = self.next_states[indices]\n",
    "#         dones = self.dones[indices].unsqueeze(1).float()\n",
    "\n",
    "#         return states, actions, rewards, next_states, dones, weights, indices\n",
    "\n",
    "#     def update_priorities(self, indices, td_errors):\n",
    "#         for idx, td in zip(indices, td_errors):\n",
    "#             self.priorities[idx] = abs(td) + 1e-6\n",
    "\n",
    "# ########### Network Definitions ###########\n",
    "# # Actor network: Gaussian policy with tanh and action mapping\n",
    "# class Actor(nn.Module):\n",
    "#     def __init__(self, state_shape, action_dim):\n",
    "#         super().__init__()\n",
    "#         c, h, w = state_shape\n",
    "#         # Conv encoder\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(c, 32, 8, 4), nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, 3, 1), nn.ReLU(),\n",
    "#             nn.Flatten())\n",
    "#         # Compute conv output size\n",
    "#         def conv_out(sz, k, s): return (sz - (k - 1) - 1) // s + 1\n",
    "#         convw = conv_out(conv_out(conv_out(w, 8, 4), 4, 2), 3, 1)\n",
    "#         convh = conv_out(conv_out(conv_out(h, 8, 4), 4, 2), 3, 1)\n",
    "#         lin = convw * convh * 64\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(lin, 512), nn.ReLU(),\n",
    "#             nn.Linear(512, 512), nn.ReLU())\n",
    "#         self.mean = nn.Linear(512, action_dim)\n",
    "#         self.log_std = nn.Linear(512, action_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x / 255.0)\n",
    "#         x = self.fc(x)\n",
    "#         mean = self.mean(x)\n",
    "#         log_std = self.log_std(x).clamp(-20, 2)\n",
    "#         return mean, log_std\n",
    "\n",
    "#     def sample(self, x):\n",
    "#         mean, log_std = self.forward(x)\n",
    "#         std = torch.exp(log_std)\n",
    "#         dist = Normal(mean, std)\n",
    "#         z = dist.rsample()\n",
    "#         raw = torch.tanh(z)\n",
    "\n",
    "#         # Map actions: steer in [-1,1], gas/brake in [0,1]\n",
    "#         steer = raw[:, 0:1]\n",
    "#         gas   = (raw[:, 1:2] + 1.0) / 2.0\n",
    "#         brake = (raw[:, 2:3] + 1.0) / 2.0\n",
    "#         action = torch.cat([steer, gas, brake], dim=1)\n",
    "\n",
    "#         # Log-prob correction\n",
    "#         logp = dist.log_prob(z) - torch.log(1 - raw.pow(2) + 1e-6)\n",
    "#         logp = logp.sum(1, keepdim=True)\n",
    "#         return action, logp\n",
    "\n",
    "# # Critic network\n",
    "# class Critic(nn.Module):\n",
    "#     def __init__(self, state_shape, action_dim):\n",
    "#         super().__init__()\n",
    "#         c, h, w = state_shape\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(c, 32, 8, 4), nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, 3, 1), nn.ReLU(),\n",
    "#             nn.Flatten())\n",
    "#         def conv_out(sz, k, s): return (sz - (k - 1) - 1) // s + 1\n",
    "#         convw = conv_out(conv_out(conv_out(w, 8, 4), 4, 2), 3, 1)\n",
    "#         convh = conv_out(conv_out(conv_out(h, 8, 4), 4, 2), 3, 1)\n",
    "#         lin = convw * convh * 64\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(lin + action_dim, 512), nn.ReLU(),\n",
    "#             nn.Linear(512, 512), nn.ReLU(),\n",
    "#             nn.Linear(512, 1))\n",
    "\n",
    "#     def forward(self, x, a):\n",
    "#         x = self.conv(x / 255.0)\n",
    "#         return self.fc(torch.cat([x, a], dim=1))\n",
    "\n",
    "# ########### Soft Updates ###########\n",
    "# def soft_update(target, source, tau):\n",
    "#     for t, s in zip(target.parameters(), source.parameters()):\n",
    "#         t.data.copy_(t.data * (1.0 - tau) + s.data * tau)\n",
    "\n",
    "# def hard_update(target, source):\n",
    "#     target.load_state_dict(source.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b888187",
   "metadata": {},
   "source": [
    "## Our Final Approach to the Problem (Race Car Behavior Optimization)\n",
    "\n",
    "#### We implemented a SAC agent augmented with a \"chain-of-thought\" LSTM (long short-term memory model), both in actor and critic, to carry out hidden memory across time and trained on a reward model ``Shaped Rewards`` from the environment. (BTW, we have kept the reward model same across all the experiments). Experience is stored in a prioritized, N-step replay buffer (our learning from experiment-2. We keep the good things.), that also keeps the LSTM's hidden states. During training, we update actor, twin critics and the temperature (how creatively the car is runninng on the road) via soft updates. The ``Reward`` we got was a significant jump from ``791.11`` to ``1514.86``. This reward satisfied us and we stopped asking the question to ourselves. CAN WE DO BETTER?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75817c",
   "metadata": {},
   "source": [
    "### Chain-of-Thought Module\n",
    "\n",
    "The `CoTModule` is a neural network module that implements an LSTM cell to maintain a hidden memory state across time steps. This module is used to augment both the actor and critic networks, enabling them to carry temporal context (chain-of-thought) during decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50426a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Chain-of-Thought Module ----------\n",
    "class CoTModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTMCell(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, hx, cx):\n",
    "        # x: [B, input_dim], hx, cx: [B, hidden_dim]\n",
    "        return self.lstm(x, (hx, cx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeabb98",
   "metadata": {},
   "source": [
    "### Prioritized Replay Buffer Module\n",
    "\n",
    "The **Prioritized N-Step Replay Buffer with CoT States** maintains a fixed-capacity circular buffer of transitions augmented with LSTM-style hidden states (`hx`, `cx`) and supports n-step return computation by temporarily storing the last _n_ steps, then aggregating discounted rewards and final next-state information when enough steps accumulate or on terminal; it samples according to priorities^α, applies importance-sampling weights annealed from β₀ to 1.0 over a schedule, and lets you update priorities based on absolute TD errors plus a small ε to ensure nonzero probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc19a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Prioritized N-Step Replay Buffer with CoT States ----------\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, state_shape, action_shape, cot_dim, device,\n",
    "                 alpha=0.6, beta_start=0.4, beta_frames=100000,\n",
    "                 n_step=3, gamma=0.99):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame_idx = 1\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        self.n_step_buffer = []\n",
    "\n",
    "        # storage\n",
    "        self.states = torch.zeros((capacity, *state_shape), device=device)\n",
    "        self.actions = torch.zeros((capacity, *action_shape), device=device)\n",
    "        self.rewards = torch.zeros((capacity,), device=device)\n",
    "        self.next_states = torch.zeros((capacity, *state_shape), device=device)\n",
    "        self.dones = torch.zeros((capacity,), device=device)\n",
    "        # CoT hidden states hx, cx\n",
    "        self.hxs = torch.zeros((capacity, cot_dim), device=device)\n",
    "        self.cxs = torch.zeros((capacity, cot_dim), device=device)\n",
    "        self.next_hxs = torch.zeros((capacity, cot_dim), device=device)\n",
    "        self.next_cxs = torch.zeros((capacity, cot_dim), device=device)\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "    def beta_by_frame(self):\n",
    "        return min(1.0, self.beta_start + (1.0 - self.beta_start) * self.frame_idx / self.beta_frames)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, hx, cx, next_hx, next_cx):\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done, hx, cx, next_hx, next_cx))\n",
    "        if len(self.n_step_buffer) < self.n_step and not done:\n",
    "            return\n",
    "        # compute N-step return\n",
    "        cum_reward, f_next_state, f_done, f_next_hx, f_next_cx = 0, None, False, None, None\n",
    "        for idx, (_, _, r, ns, d, _, _, nhx, ncx) in enumerate(self.n_step_buffer):\n",
    "            cum_reward += (self.gamma**idx) * r\n",
    "            f_next_state, f_done, f_next_hx, f_next_cx = ns, d, nhx, ncx\n",
    "            if d:\n",
    "                break\n",
    "        s, a, _, _, _, h0, c0, _, _ = self.n_step_buffer[0]\n",
    "        pos = self.position\n",
    "        self.states[pos] = torch.tensor(s, device=self.device)\n",
    "        self.actions[pos] = torch.tensor(a, device=self.device)\n",
    "        self.rewards[pos] = cum_reward\n",
    "        self.next_states[pos] = torch.tensor(f_next_state, device=self.device)\n",
    "        self.dones[pos] = f_done\n",
    "        self.hxs[pos] = hx\n",
    "        self.cxs[pos] = cx\n",
    "        self.next_hxs[pos] = f_next_hx\n",
    "        self.next_cxs[pos] = f_next_cx\n",
    "        self.priorities[pos] = self.priorities.max() if self.size>0 else 1.0\n",
    "        self.position = (pos+1) % self.capacity\n",
    "        self.size = min(self.size+1, self.capacity)\n",
    "        self.n_step_buffer.pop(0)\n",
    "        if done:\n",
    "            self.n_step_buffer.clear()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        assert self.size>0\n",
    "        self.frame_idx +=1\n",
    "        prios = self.priorities[:self.size] + 1e-6\n",
    "        probs = prios**self.alpha\n",
    "        probs /= probs.sum()\n",
    "        idxs = np.random.choice(self.size, batch_size, p=probs)\n",
    "        beta = self.beta_by_frame()\n",
    "        weights = (self.size * probs[idxs])**(-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.tensor(weights, device=self.device)\n",
    "\n",
    "        batch = dict(\n",
    "            s=self.states[idxs],\n",
    "            a=self.actions[idxs],\n",
    "            r=self.rewards[idxs].unsqueeze(1),\n",
    "            s2=self.next_states[idxs],\n",
    "            d=self.dones[idxs].unsqueeze(1).float(),\n",
    "            hx=self.hxs[idxs],\n",
    "            cx=self.cxs[idxs],\n",
    "            hx2=self.next_hxs[idxs],\n",
    "            cx2=self.next_cxs[idxs],\n",
    "            w=weights.unsqueeze(1),\n",
    "            idxs=idxs\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "    def update_priorities(self, idxs, td_errors):\n",
    "        for i, td in zip(idxs, td_errors):\n",
    "            self.priorities[i] = abs(td) + 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b70bd",
   "metadata": {},
   "source": [
    "### Soft Actor Critic\n",
    "\n",
    "The following code defines the neural network architectures for the Actor and Critic used in the SAC agent with Chain-of-Thought (CoT) LSTM modules. Both networks use convolutional layers for feature extraction, followed by fully connected layers and an LSTM cell to maintain temporal context. The Actor outputs the mean and log standard deviation for a Gaussian policy, while the Critic estimates the Q-value for state-action pairs.\n",
    "\n",
    "- **Actor**: Processes the state through convolutional and fully connected layers, then passes the result through a CoT LSTM. The output is used to parameterize a Gaussian distribution, from which actions are sampled and mapped to the environment's action space.\n",
    "- **Critic**: Processes the state and action together through convolutional and fully connected layers, then passes the result through a CoT LSTM to estimate the Q-value.\n",
    "\n",
    "Both classes provide methods to initialize the LSTM hidden states and to perform forward passes with temporal memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60698bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Networks ----------\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_shape, action_dim, cot_dim=256):\n",
    "        super().__init__()\n",
    "        c,h,w = state_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c,32,8,4), nn.ReLU(),\n",
    "            nn.Conv2d(32,64,4,2), nn.ReLU(),\n",
    "            nn.Conv2d(64,64,3,1), nn.ReLU(),\n",
    "            nn.Flatten())\n",
    "        def conv_out(sz,k,s): return (sz-(k-1)-1)//s+1\n",
    "        convw = conv_out(conv_out(conv_out(w,8,4),4,2),3,1)\n",
    "        convh = conv_out(conv_out(conv_out(h,8,4),4,2),3,1)\n",
    "        lin = convw*convh*64\n",
    "        self.fc_enc = nn.Sequential(nn.Linear(lin,512), nn.ReLU())\n",
    "        self.cot = CoTModule(512, cot_dim)\n",
    "        self.fc_head = nn.Sequential(nn.Linear(cot_dim,512), nn.ReLU())\n",
    "        self.mean = nn.Linear(512, action_dim)\n",
    "        self.log_std = nn.Linear(512, action_dim)\n",
    "\n",
    "    def init_hidden(self, bsz, device):\n",
    "        return (torch.zeros(bsz, self.cot.lstm.hidden_size, device=device),\n",
    "                torch.zeros(bsz, self.cot.lstm.hidden_size, device=device))\n",
    "\n",
    "    def forward(self, x, hx, cx):\n",
    "        feat = self.conv(x/255.0)\n",
    "        enc = self.fc_enc(feat)\n",
    "        hx, cx = self.cot(enc, hx, cx)\n",
    "        h = self.fc_head(hx)\n",
    "        mean = self.mean(h)\n",
    "        log_std = self.log_std(h).clamp(-20,2)\n",
    "        return mean, log_std, hx, cx\n",
    "\n",
    "    def sample(self, x, hx, cx):\n",
    "        mean, log_std, hx, cx = self.forward(x, hx, cx)\n",
    "        std = torch.exp(log_std)\n",
    "        dist = Normal(mean, std)\n",
    "        z = dist.rsample()\n",
    "        raw = torch.tanh(z)\n",
    "        steer = raw[:,0:1]\n",
    "        gas   = (raw[:,1:2]+1)/2\n",
    "        brake = (raw[:,2:3]+1)/2\n",
    "        action = torch.cat([steer,gas,brake],1)\n",
    "        logp = dist.log_prob(z) - torch.log(1-raw.pow(2)+1e-6)\n",
    "        return action, logp.sum(1,True), hx, cx\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_shape, action_dim, cot_dim=256):\n",
    "        super().__init__()\n",
    "        c,h,w = state_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c,32,8,4), nn.ReLU(),\n",
    "            nn.Conv2d(32,64,4,2), nn.ReLU(),\n",
    "            nn.Conv2d(64,64,3,1), nn.ReLU(),\n",
    "            nn.Flatten())\n",
    "        def conv_out(sz,k,s): return (sz-(k-1)-1)//s+1\n",
    "        convw = conv_out(conv_out(conv_out(w,8,4),4,2),3,1)\n",
    "        convh = conv_out(conv_out(conv_out(h,8,4),4,2),3,1)\n",
    "        lin = convw*convh*64 + action_dim\n",
    "        self.fc_enc = nn.Sequential(nn.Linear(lin,512), nn.ReLU())\n",
    "        self.cot = CoTModule(512, cot_dim)\n",
    "        self.fc_out = nn.Linear(cot_dim,1)\n",
    "\n",
    "    def init_hidden(self, bsz, device):\n",
    "        return (torch.zeros(bsz, self.cot.lstm.hidden_size, device=device),\n",
    "                torch.zeros(bsz, self.cot.lstm.hidden_size, device=device))\n",
    "\n",
    "    def forward(self, x, a, hx, cx):\n",
    "        feat = self.conv(x/255.0)\n",
    "        enc = torch.cat([feat, a],1)\n",
    "        enc = self.fc_enc(enc)\n",
    "        hx, cx = self.cot(enc, hx, cx)\n",
    "        q = self.fc_out(hx)\n",
    "        return q, hx, cx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa70c4",
   "metadata": {},
   "source": [
    "### Reward Shaping and Soft Updates\n",
    "\n",
    "The `ShapedReward` class is a custom Gym `RewardWrapper` that augments the environment's reward signal with additional terms to encourage forward velocity, penalize driving on grass, and discourage excessive rotation. This helps guide the agent toward more desirable behaviors.\n",
    "\n",
    "The `soft_update` and `hard_update` functions are utility methods for updating the target networks in soft actor-critic (SAC) algorithms. `soft_update` performs a weighted update of the target network parameters, while `hard_update` copies the parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23acfd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Reward Shaping ----------\n",
    "class ShapedReward(RewardWrapper):\n",
    "    def __init__(self, env): super().__init__(env)\n",
    "    def reward(self, r):\n",
    "        un = self.unwrapped\n",
    "        vel = un.car.hull.linearVelocity\n",
    "        ang_v = un.car.hull.angularVelocity\n",
    "        angle = un.car.hull.angle\n",
    "        forward = vel.x*math.cos(angle) + vel.y*math.sin(angle)\n",
    "        frame = un.render()\n",
    "        h,w,_=frame.shape; cx,cy=w//2,h//2\n",
    "        r_pix,g_pix,b_pix=frame[cy,cx]\n",
    "        grass_penalty = -0.2 if (g_pix>150 and g_pix>r_pix+30 and g_pix>b_pix+30) else 0.0\n",
    "        rot_penalty = -0.05*abs(ang_v)\n",
    "        shaped = 0.1*max(0,forward) + grass_penalty + rot_penalty\n",
    "        return r + shaped\n",
    "\n",
    "# ---------- Soft Updates ----------\n",
    "def soft_update(tgt, src, tau):\n",
    "    for t,s in zip(tgt.parameters(), src.parameters()):\n",
    "        t.data.copy_(t.data*(1-tau) + s.data*tau)\n",
    "def hard_update(tgt, src): tgt.load_state_dict(src.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e7e56",
   "metadata": {},
   "source": [
    "### Our Training Regime\n",
    "\n",
    "Here we are checking if the reward has improved after every 10 episodes. If the reward has improved, we are saving the model. If not, then we continue traning the model for 2000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ddabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VARNIT MITTAL\\AppData\\Local\\Temp\\ipykernel_21260\\1747919756.py:253: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  state = np.array(obs)\n",
      "C:\\Users\\VARNIT MITTAL\\AppData\\Local\\Temp\\ipykernel_21260\\1747919756.py:269: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  nxt = np.array(obs2)\n",
      "C:\\Users\\VARNIT MITTAL\\AppData\\Local\\Temp\\ipykernel_21260\\1747919756.py:117: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.priorities[i] = abs(td) + 1e-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep   10 avg10  -68.86 best -1000000000.00\n",
      "Saved best model\n",
      "Ep   20 avg10  -75.76 best  -68.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VARNIT MITTAL\\AppData\\Local\\Temp\\ipykernel_21260\\1747919756.py:206: RuntimeWarning: overflow encountered in scalar add\n",
      "  grass_penalty = -0.2 if (g_pix>150 and g_pix>r_pix+30 and g_pix>b_pix+30) else 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep   30 avg10  -78.20 best  -68.86\n",
      "Ep   40 avg10  -69.11 best  -68.86\n",
      "Ep   50 avg10  -68.77 best  -68.86\n",
      "Saved best model\n",
      "Ep   60 avg10  -70.17 best  -68.77\n",
      "Ep   70 avg10  -65.08 best  -68.77\n",
      "Saved best model\n",
      "Ep   80 avg10  -85.28 best  -65.08\n",
      "Ep   90 avg10  -72.62 best  -65.08\n",
      "Ep  100 avg10  -51.50 best  -65.08\n",
      "Saved best model\n",
      "Ep  110 avg10  -61.70 best  -51.50\n",
      "Ep  120 avg10  -75.45 best  -51.50\n",
      "Ep  130 avg10 -111.79 best  -51.50\n",
      "Ep  140 avg10 -132.76 best  -51.50\n",
      "Ep  150 avg10  -66.81 best  -51.50\n",
      "Ep  160 avg10  -17.26 best  -51.50\n",
      "Saved best model\n",
      "Ep  170 avg10   72.90 best  -17.26\n",
      "Saved best model\n",
      "Ep  180 avg10  193.02 best   72.90\n",
      "Saved best model\n",
      "Ep  190 avg10  298.05 best  193.02\n",
      "Saved best model\n",
      "Ep  200 avg10  297.15 best  298.05\n",
      "Ep  210 avg10  271.07 best  298.05\n",
      "Ep  220 avg10  273.97 best  298.05\n",
      "Ep  230 avg10  247.29 best  298.05\n",
      "Ep  240 avg10  244.15 best  298.05\n",
      "Ep  250 avg10  244.99 best  298.05\n",
      "Ep  260 avg10  241.78 best  298.05\n",
      "Ep  270 avg10  232.42 best  298.05\n",
      "Ep  280 avg10  233.66 best  298.05\n",
      "Ep  290 avg10  222.49 best  298.05\n",
      "Ep  300 avg10  217.56 best  298.05\n",
      "Ep  310 avg10  213.75 best  298.05\n",
      "Ep  320 avg10  240.64 best  298.05\n",
      "Ep  330 avg10  232.98 best  298.05\n",
      "Ep  340 avg10  245.44 best  298.05\n",
      "Ep  350 avg10  224.16 best  298.05\n",
      "Ep  360 avg10  224.24 best  298.05\n",
      "Ep  370 avg10  229.77 best  298.05\n",
      "Ep  380 avg10  231.85 best  298.05\n",
      "Ep  390 avg10  230.41 best  298.05\n",
      "Ep  400 avg10  238.49 best  298.05\n",
      "Ep  410 avg10  231.75 best  298.05\n",
      "Ep  420 avg10  238.39 best  298.05\n",
      "Ep  430 avg10  245.34 best  298.05\n",
      "Ep  440 avg10  257.63 best  298.05\n",
      "Ep  450 avg10  254.48 best  298.05\n",
      "Ep  460 avg10  254.96 best  298.05\n",
      "Ep  470 avg10  270.31 best  298.05\n",
      "Ep  480 avg10  264.76 best  298.05\n",
      "Ep  490 avg10  262.24 best  298.05\n",
      "Ep  500 avg10  256.72 best  298.05\n",
      "Ep  510 avg10  271.57 best  298.05\n",
      "Ep  520 avg10  270.69 best  298.05\n",
      "Ep  530 avg10  271.73 best  298.05\n",
      "Ep  540 avg10  286.16 best  298.05\n",
      "Ep  550 avg10  272.51 best  298.05\n",
      "Ep  560 avg10  275.64 best  298.05\n",
      "Ep  570 avg10  277.63 best  298.05\n",
      "Ep  580 avg10  290.22 best  298.05\n",
      "Ep  590 avg10  301.62 best  298.05\n",
      "Saved best model\n",
      "Ep  600 avg10  292.44 best  301.62\n",
      "Ep  610 avg10  291.57 best  301.62\n",
      "Ep  620 avg10  317.65 best  301.62\n",
      "Saved best model\n",
      "Ep  630 avg10  300.09 best  317.65\n",
      "Ep  640 avg10  317.05 best  317.65\n",
      "Ep  650 avg10  308.13 best  317.65\n",
      "Ep  660 avg10  310.48 best  317.65\n",
      "Ep  670 avg10  308.19 best  317.65\n",
      "Ep  680 avg10  312.36 best  317.65\n",
      "Ep  690 avg10  319.26 best  317.65\n",
      "Saved best model\n",
      "Ep  700 avg10  316.24 best  319.26\n",
      "Ep  710 avg10  323.71 best  319.26\n",
      "Saved best model\n",
      "Ep  720 avg10  327.24 best  323.71\n",
      "Saved best model\n",
      "Ep  730 avg10  338.44 best  327.24\n",
      "Saved best model\n",
      "Ep  740 avg10  340.04 best  338.44\n",
      "Saved best model\n",
      "Ep  750 avg10  353.28 best  340.04\n",
      "Saved best model\n",
      "Ep  760 avg10  350.06 best  353.28\n",
      "Ep  770 avg10  356.10 best  353.28\n",
      "Saved best model\n",
      "Ep  780 avg10  365.48 best  356.10\n",
      "Saved best model\n",
      "Ep  790 avg10  369.26 best  365.48\n",
      "Saved best model\n",
      "Ep  800 avg10  373.50 best  369.26\n",
      "Saved best model\n",
      "Ep  810 avg10  382.54 best  373.50\n",
      "Saved best model\n",
      "Ep  820 avg10  381.59 best  382.54\n",
      "Ep  830 avg10  387.69 best  382.54\n",
      "Saved best model\n",
      "Ep  840 avg10  399.80 best  387.69\n",
      "Saved best model\n",
      "Ep  850 avg10  389.54 best  399.80\n",
      "Ep  860 avg10  404.34 best  399.80\n",
      "Saved best model\n",
      "Ep  870 avg10  399.24 best  404.34\n",
      "Ep  880 avg10  411.81 best  404.34\n",
      "Saved best model\n",
      "Ep  890 avg10  411.70 best  411.81\n",
      "Ep  900 avg10  415.05 best  411.81\n",
      "Saved best model\n",
      "Ep  910 avg10  421.41 best  415.05\n",
      "Saved best model\n",
      "Ep  920 avg10  423.78 best  421.41\n",
      "Saved best model\n",
      "Ep  930 avg10  425.15 best  423.78\n",
      "Saved best model\n",
      "Ep  940 avg10  449.87 best  425.15\n",
      "Saved best model\n",
      "Ep  950 avg10  440.72 best  449.87\n",
      "Ep  960 avg10  445.91 best  449.87\n",
      "Ep  970 avg10  490.80 best  449.87\n",
      "Saved best model\n",
      "Ep  980 avg10  522.85 best  490.80\n",
      "Saved best model\n",
      "Ep  990 avg10  607.76 best  522.85\n",
      "Saved best model\n",
      "Ep 1000 avg10  600.61 best  607.76\n",
      "Ep 1010 avg10  681.67 best  607.76\n",
      "Saved best model\n",
      "Ep 1020 avg10  780.68 best  681.67\n",
      "Saved best model\n",
      "Ep 1030 avg10  842.77 best  780.68\n",
      "Saved best model\n",
      "Ep 1040 avg10  703.60 best  842.77\n",
      "Ep 1050 avg10  744.79 best  842.77\n",
      "Ep 1060 avg10  812.71 best  842.77\n",
      "Ep 1070 avg10  761.66 best  842.77\n",
      "Ep 1080 avg10  799.60 best  842.77\n",
      "Ep 1090 avg10  832.23 best  842.77\n",
      "Ep 1100 avg10  855.59 best  842.77\n",
      "Saved best model\n",
      "Ep 1110 avg10  880.00 best  855.59\n",
      "Saved best model\n",
      "Ep 1120 avg10  778.45 best  880.00\n",
      "Ep 1130 avg10  830.73 best  880.00\n",
      "Ep 1140 avg10  894.65 best  880.00\n",
      "Saved best model\n",
      "Ep 1150 avg10  924.53 best  894.65\n",
      "Saved best model\n",
      "Ep 1160 avg10  950.33 best  924.53\n",
      "Saved best model\n",
      "Ep 1170 avg10 1033.91 best  950.33\n",
      "Saved best model\n",
      "Ep 1180 avg10  995.78 best 1033.91\n",
      "Ep 1190 avg10 1084.27 best 1033.91\n",
      "Saved best model\n",
      "Ep 1200 avg10 1050.98 best 1084.27\n",
      "Ep 1210 avg10 1073.02 best 1084.27\n",
      "Ep 1220 avg10 1130.94 best 1084.27\n",
      "Saved best model\n",
      "Ep 1230 avg10 1161.64 best 1130.94\n",
      "Saved best model\n",
      "Ep 1240 avg10 1072.94 best 1161.64\n",
      "Ep 1250 avg10 1162.54 best 1161.64\n",
      "Saved best model\n",
      "Ep 1260 avg10 1079.04 best 1162.54\n",
      "Ep 1270 avg10 1220.21 best 1162.54\n",
      "Saved best model\n",
      "Ep 1280 avg10 1271.31 best 1220.21\n",
      "Saved best model\n",
      "Ep 1290 avg10 1272.81 best 1271.31\n",
      "Saved best model\n",
      "Ep 1300 avg10 1273.66 best 1272.81\n",
      "Saved best model\n",
      "Ep 1310 avg10 1291.02 best 1273.66\n",
      "Saved best model\n",
      "Ep 1320 avg10 1305.86 best 1291.02\n",
      "Saved best model\n",
      "Ep 1330 avg10 1300.59 best 1305.86\n",
      "Ep 1340 avg10 1282.12 best 1305.86\n",
      "Ep 1350 avg10 1304.59 best 1305.86\n",
      "Ep 1360 avg10 1364.51 best 1305.86\n",
      "Saved best model\n",
      "Ep 1370 avg10 1365.56 best 1364.51\n",
      "Saved best model\n",
      "Ep 1380 avg10 1424.78 best 1365.56\n",
      "Saved best model\n",
      "Ep 1390 avg10 1378.23 best 1424.78\n",
      "Ep 1400 avg10 1436.01 best 1424.78\n",
      "Saved best model\n",
      "Ep 1410 avg10 1399.71 best 1436.01\n",
      "Ep 1420 avg10 1412.57 best 1436.01\n",
      "Ep 1430 avg10 1325.87 best 1436.01\n",
      "Ep 1440 avg10 1471.76 best 1436.01\n",
      "Saved best model\n",
      "Ep 1450 avg10 1468.31 best 1471.76\n",
      "Ep 1460 avg10 1369.78 best 1471.76\n",
      "Ep 1470 avg10 1514.86 best 1471.76\n",
      "Saved best model\n",
      "Ep 1480 avg10 1412.12 best 1514.86\n",
      "Ep 1490 avg10 1455.53 best 1514.86\n",
      "Ep 1500 avg10 1421.19 best 1514.86\n",
      "Ep 1510 avg10 1411.33 best 1514.86\n",
      "Ep 1520 avg10 1389.13 best 1514.86\n",
      "Ep 1530 avg10 1423.37 best 1514.86\n",
      "Ep 1540 avg10 1334.46 best 1514.86\n",
      "Ep 1550 avg10 1396.36 best 1514.86\n",
      "Ep 1560 avg10 1434.96 best 1514.86\n",
      "Ep 1570 avg10 1440.66 best 1514.86\n",
      "Ep 1580 avg10 1450.43 best 1514.86\n",
      "Ep 1590 avg10 1457.91 best 1514.86\n",
      "Ep 1600 avg10 1379.56 best 1514.86\n",
      "Ep 1610 avg10 1348.41 best 1514.86\n",
      "Ep 1620 avg10 1432.63 best 1514.86\n",
      "Ep 1630 avg10 1397.43 best 1514.86\n",
      "Ep 1640 avg10 1372.74 best 1514.86\n",
      "Ep 1650 avg10 1414.17 best 1514.86\n",
      "Ep 1660 avg10 1463.21 best 1514.86\n",
      "Ep 1670 avg10 1376.01 best 1514.86\n",
      "Ep 1680 avg10 1357.91 best 1514.86\n",
      "Ep 1690 avg10 1439.69 best 1514.86\n",
      "Ep 1700 avg10 1433.48 best 1514.86\n",
      "Ep 1710 avg10 1399.68 best 1514.86\n",
      "Ep 1720 avg10 1288.10 best 1514.86\n",
      "Ep 1730 avg10 1419.16 best 1514.86\n",
      "Ep 1740 avg10 1429.47 best 1514.86\n",
      "Ep 1750 avg10 1366.27 best 1514.86\n",
      "Ep 1760 avg10 1418.86 best 1514.86\n",
      "Ep 1770 avg10 1392.73 best 1514.86\n",
      "Ep 1780 avg10 1335.97 best 1514.86\n",
      "Ep 1790 avg10 1383.28 best 1514.86\n",
      "Ep 1800 avg10 1348.92 best 1514.86\n",
      "Ep 1810 avg10 1345.92 best 1514.86\n",
      "Ep 1820 avg10 1299.31 best 1514.86\n",
      "Ep 1830 avg10 1310.49 best 1514.86\n",
      "Ep 1840 avg10 1448.29 best 1514.86\n",
      "Ep 1850 avg10 1348.58 best 1514.86\n",
      "Ep 1860 avg10 1330.47 best 1514.86\n",
      "Ep 1870 avg10 1369.19 best 1514.86\n",
      "Ep 1880 avg10 1341.17 best 1514.86\n",
      "Ep 1890 avg10 1415.43 best 1514.86\n",
      "Ep 1900 avg10 1400.24 best 1514.86\n",
      "Ep 1910 avg10 1372.78 best 1514.86\n",
      "Ep 1920 avg10 1353.03 best 1514.86\n",
      "Ep 1930 avg10 1468.44 best 1514.86\n",
      "Ep 1940 avg10 1360.08 best 1514.86\n",
      "Ep 1950 avg10 1356.58 best 1514.86\n",
      "Ep 1960 avg10 1334.42 best 1514.86\n",
      "Ep 1970 avg10 1278.15 best 1514.86\n",
      "Ep 1980 avg10 1364.95 best 1514.86\n",
      "Ep 1990 avg10 1371.93 best 1514.86\n",
      "Ep 2000 avg10 1354.05 best 1514.86\n"
     ]
    }
   ],
   "source": [
    "# ---------- Training ----------\n",
    "def train():\n",
    "    # Hyperparams\n",
    "    env_name=\"CarRacing-v2\"; max_episodes=2000; max_steps=1000\n",
    "    batch_size=64; gamma=0.99; tau=0.005; lr=2e-4\n",
    "    buffer_size=200000; n_step=3; cot_dim=256\n",
    "\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    env = ShapedReward(env)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env,(84,84))\n",
    "    env = FrameStack(env,4)\n",
    "    state_shape=(4,84,84); action_dim=env.action_space.shape[0]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # models\n",
    "    actor = Actor(state_shape, action_dim, cot_dim).to(device)\n",
    "    critic1 = Critic(state_shape, action_dim, cot_dim).to(device)\n",
    "    critic2 = Critic(state_shape, action_dim, cot_dim).to(device)\n",
    "    c1_tgt = Critic(state_shape, action_dim, cot_dim).to(device)\n",
    "    c2_tgt = Critic(state_shape, action_dim, cot_dim).to(device)\n",
    "    hard_update(c1_tgt, critic1); hard_update(c2_tgt, critic2)\n",
    "\n",
    "    actor_opt = optim.Adam(actor.parameters(), lr=lr)\n",
    "    c1_opt = optim.Adam(critic1.parameters(), lr=lr)\n",
    "    c2_opt = optim.Adam(critic2.parameters(), lr=lr)\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha_opt = optim.Adam([log_alpha], lr=lr)\n",
    "    target_entropy = -action_dim\n",
    "\n",
    "    buffer = PrioritizedReplayBuffer(buffer_size, state_shape, (action_dim,), cot_dim, device,\n",
    "                                     alpha=0.6, n_step=n_step, gamma=gamma)\n",
    "\n",
    "    best_avg=-1e9; history=[]\n",
    "    for ep in range(1, max_episodes+1):\n",
    "        obs = env.reset() if not isinstance(env.reset(), tuple) else env.reset()[0]\n",
    "        state = np.array(obs)\n",
    "        if state.ndim==4 and state.shape[-1]==1: state=state.squeeze(-1)\n",
    "        # init CoT\n",
    "        hx, cx = actor.init_hidden(1, device)\n",
    "        ep_r=0\n",
    "        for t in range(max_steps):\n",
    "            st = torch.tensor(state, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                a, lp, nhx, ncx = actor.sample(st, hx, cx)\n",
    "            action = a.cpu().numpy()[0]\n",
    "            # step\n",
    "            total_r=0; done=False\n",
    "            for _ in range(4):\n",
    "                obs2, r, done_flag, trunc, _=env.step(action.tolist())\n",
    "                total_r+=r\n",
    "                if done_flag or trunc: done=True; break\n",
    "            nxt = np.array(obs2)\n",
    "            if nxt.ndim==4 and nxt.shape[-1]==1: nxt=nxt.squeeze(-1)\n",
    "\n",
    "            # store with CoT states\n",
    "            buffer.add(state, action, total_r, nxt, done,\n",
    "                       hx.squeeze(0), cx.squeeze(0), nhx.squeeze(0), ncx.squeeze(0))\n",
    "            state, hx, cx = nxt, nhx, ncx\n",
    "            ep_r+=total_r\n",
    "\n",
    "            if buffer.size>=batch_size:\n",
    "                batch = buffer.sample(batch_size)\n",
    "                # critic update\n",
    "                with torch.no_grad():\n",
    "                    a2, lp2, hx2, cx2 = actor.sample(batch['s2'], batch['hx2'], batch['cx2'])\n",
    "                    q1_t,_,_ = c1_tgt(batch['s2'], a2, batch['hx2'], batch['cx2'])\n",
    "                    q2_t,_,_ = c2_tgt(batch['s2'], a2, batch['hx2'], batch['cx2'])\n",
    "                    q_min = torch.min(q1_t, q2_t) - log_alpha.exp()*lp2\n",
    "                    target_q = batch['r'] + (1-batch['d'])*(gamma**n_step)*q_min\n",
    "\n",
    "                q1, _, _ = critic1(batch['s'], batch['a'], batch['hx'], batch['cx'])\n",
    "                q2, _, _ = critic2(batch['s'], batch['a'], batch['hx'], batch['cx'])\n",
    "                td1 = q1 - target_q; td2 = q2 - target_q\n",
    "                loss_c1 = (batch['w'] * td1.pow(2)).mean()\n",
    "                loss_c2 = (batch['w'] * td2.pow(2)).mean()\n",
    "                c1_opt.zero_grad(); loss_c1.backward(); c1_opt.step()\n",
    "                c2_opt.zero_grad(); loss_c2.backward(); c2_opt.step()\n",
    "                buffer.update_priorities(batch['idxs'], ((td1+td2)/2).abs().detach().cpu().numpy())\n",
    "                # actor update\n",
    "                a_new, lp_new, _, _ = actor.sample(batch['s'], batch['hx'], batch['cx'])\n",
    "                q1_pi,_,_ = critic1(batch['s'], a_new, batch['hx'], batch['cx'])\n",
    "                q2_pi,_,_ = critic2(batch['s'], a_new, batch['hx'], batch['cx'])\n",
    "                q_pi = torch.min(q1_pi, q2_pi)\n",
    "                alpha = log_alpha.exp()\n",
    "                loss_pi = (alpha * lp_new - q_pi).mean()\n",
    "                actor_opt.zero_grad(); loss_pi.backward(); actor_opt.step()\n",
    "                # alpha update\n",
    "                loss_a = -(log_alpha * (lp_new + target_entropy).detach()).mean()\n",
    "                alpha_opt.zero_grad(); loss_a.backward(); alpha_opt.step()\n",
    "                soft_update(c1_tgt, critic1, tau); soft_update(c2_tgt, critic2, tau)\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        history.append(ep_r)\n",
    "        if ep%10==0:\n",
    "            avg10 = sum(history[-10:])/len(history[-10:])\n",
    "            print(f\"Ep {ep:4d} avg10 {avg10:7.2f} best {best_avg:7.2f}\")\n",
    "            if avg10>best_avg:\n",
    "                best_avg=avg10; os.makedirs(\"ckpt\", exist_ok=True)\n",
    "                torch.save(actor.state_dict(), \"ckpt/actor.pth\")\n",
    "                print(\"Saved best model\")\n",
    "    env.close()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please_help",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
